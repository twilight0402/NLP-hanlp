{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85584\n",
      "煤场\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "\n",
    "def load_dictionary():\n",
    "    \"\"\"\n",
    "    加载HanLP中的mini词库\n",
    "    :return: 一个set形式的词库\n",
    "    \"\"\"\n",
    "    IOUtil = JClass('com.hankcs.hanlp.corpus.io.IOUtil')\n",
    "    # 换成一个较小的词库\n",
    "    path = HanLP.Config.CoreDictionaryPath.replace('.txt', '.mini.txt')\n",
    "    dic = IOUtil.loadDictionary([path])\n",
    "    return set(dic.keySet())\n",
    "\n",
    "\n",
    "dic = load_dictionary()\n",
    "print(len(dic))\n",
    "print(list(dic)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切分算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完全切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['商', '商品', '品', '和', '和服', '服', '服务', '务']\n"
     ]
    }
   ],
   "source": [
    "# 完全切分\n",
    "def fully_segment(text, dic):\n",
    "    word_list = []\n",
    "    # [0, len - 1]\n",
    "    for i in range(len(text)):                  # i 从 0 到text的最后一个字的下标遍历\n",
    "        # [i+1, len + 1)\n",
    "        for j in range(i + 1, len(text) + 1):   # j 遍历[i + 1, len(text)]区间\n",
    "            # [i,j)\n",
    "            word = text[i:j]                    # 取出连续区间[i, j]对应的字符串\n",
    "            if word in dic:                     # 如果在词典中，则认为是一个词\n",
    "                word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "\n",
    "dic = load_dictionary()\n",
    "print(fully_segment('商品和服务', dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最长匹配算法\n",
    "\n",
    "### 正向最长匹配\n",
    "```\n",
    "i----------j\n",
    "i=0\n",
    "i不动， j从 i+1 ==》 size\n",
    "每次选最长的ij(在字典中的)\n",
    "i+=len(ij)\n",
    "```\n",
    "\n",
    "### 逆向最长匹配\n",
    "```\n",
    "j----------i\n",
    "i=size-1\n",
    "i不动， j从 0 ==》 i-1\n",
    "每次选最长的ij\n",
    "i-=len(ij)\n",
    "```\n",
    "\n",
    "### 双向最长匹配\n",
    "- 同时执行正向和逆向最长匹配，若两者的词数不同，则返回词数更少的那一个。\n",
    "- 否则，返回两者中单字更少的那一个。当单字数也相同时，优先返回逆向最长匹配的结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正向最长匹配\n",
    "def forward_segment(text, dic):\n",
    "    word_list = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        longest_word = text[i]                      # 当前扫描位置的单字\n",
    "        for j in range(i + 1, len(text) + 1):       # 所有可能的结尾\n",
    "            word = text[i:j]                        # 从当前位置到结尾的连续字符串\n",
    "            if word in dic:                         # 在词典中\n",
    "                if len(word) > len(longest_word):   # 并且更长\n",
    "                    longest_word = word             # 则更优先输出\n",
    "        word_list.append(longest_word)              # 输出最长词\n",
    "        i += len(longest_word)                      # 正向扫描\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逆向最长匹配\n",
    "def backward_segment(text, dic):\n",
    "    word_list = []\n",
    "    i = len(text) - 1\n",
    "    while i >= 0:                                   # 扫描位置作为终点\n",
    "        longest_word = text[i]                      # 扫描位置的单字\n",
    "        for j in range(0, i):                       # 遍历[0, i]区间作为待查询词语的起点\n",
    "            word = text[j: i + 1]                   # 取出[j, i]区间作为待查询单词\n",
    "            if word in dic:\n",
    "                if len(word) > len(longest_word):   # 越长优先级越高\n",
    "                    longest_word = word\n",
    "                    break\n",
    "        word_list.insert(0, longest_word)           # 逆向扫描，所以越先查出的单词在位置上越靠后\n",
    "        i -= len(longest_word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双向最长匹配\n",
    "def bidirectional_segment(text, dic):\n",
    "    f = forward_segment(text, dic)\n",
    "    b = backward_segment(text, dic)\n",
    "    if len(f) < len(b):                                  # 词数更少优先级更高\n",
    "        return f\n",
    "    elif len(f) > len(b):\n",
    "        return b\n",
    "    else:\n",
    "        if count_single_char(f) < count_single_char(b):  # 单字更少优先级更高\n",
    "            return f\n",
    "        else:\n",
    "            return b                                     # 都相等时逆向匹配优先级更高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字典树（前缀树）\n",
    "用每个字作为一条路径（节点之间的边）\n",
    "\n",
    "## 首字散列的二分字典树\n",
    "首字使用散列表，其余节点使用二分查找 BinTree\n",
    "\n",
    "## 双数组字典树\n",
    "`DoubleArrayTrie` `parseText()` 全切分 `parseLongestText()`正向最长切分\n",
    "\n",
    "# AC自动机\n",
    "- `goto表(success表)` : 一个前缀树\n",
    "- `output表` : 表示哪些状态对应的字符串可以输出（存在于语料库中）。当前路径的后缀中，合法的后缀也会被加入到output表中（比如she， 和 he）\n",
    "- `fail表` : 状态转移失败后，应当回退的最佳状态.（回退到已匹配字符串的最长后缀上）\n",
    "\n",
    "```python\n",
    "from pyhanlp import *\n",
    "words = [\"hers\", \"his\", \"she\", \"he\"]\n",
    "ACTrie = JClass('com.hankcs.hanlp.algorithm.ahocorasick.trie.Trie')\n",
    "trie = ACTrie()\n",
    "for word in words:\n",
    "    trie.addKeyword(word)\n",
    "\n",
    "for emit in trie.parseText(\"ushers\"):\n",
    "    print(\"[%d:%d]=%s\" % (emit.getStart(), emit.getEnd(), emit.getKeyword()))\n",
    "```\n",
    "\n",
    "# 双数组字典树AC自动机\n",
    "```python\n",
    "# 双数组字典树AC自动机\n",
    "words = [\"hers\", \"his\", \"she\", \"he\"]\n",
    "map = JClass('java.util.TreeMap')()\n",
    "for word in words:\n",
    "    map[word] = word.upper()\n",
    "\n",
    "acdTrie = JClass('com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie')(map)\n",
    "\n",
    "for emit in acdTrie.parseText(\"ushers\"):\n",
    "    print(\"[%d:%d]=%s\" % (emit.begin, emit.end, emit.value))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单的字典树\n",
    "class Node(object):\n",
    "    def __init__(self, value) -> None:\n",
    "        self._children = {}\n",
    "        self._value = value\n",
    "\n",
    "    def _add_child(self, char, value, overwrite=False):\n",
    "        child = self._children.get(char)\n",
    "        if child is None:\n",
    "            child = Node(value)\n",
    "            self._children[char] = child\n",
    "        elif overwrite:\n",
    "            child._value = value\n",
    "        return child\n",
    "\n",
    "\n",
    "class Trie(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(None)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return self[key] is not None\n",
    "\n",
    "    # key 就是中文字符串\n",
    "    def __getitem__(self, key):\n",
    "        state = self\n",
    "        # char 是每一个字\n",
    "        for char in key:\n",
    "            state = state._children.get(char)\n",
    "            if state is None:\n",
    "                return None\n",
    "        return state._value\n",
    "\n",
    "    # key 是中文字符串\n",
    "    def __setitem__(self, key, value):\n",
    "        state = self\n",
    "        for i, char in enumerate(key):\n",
    "            if i < len(key) - 1:\n",
    "                # 往下递归\n",
    "                state = state._add_child(char, None, False)\n",
    "            else:\n",
    "                # 增加子节点\n",
    "                state = state._add_child(char, value, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = Trie()\n",
    "\n",
    "# 增\n",
    "# hash()\n",
    "trie['自然'] = 'nature'\n",
    "trie['自然人'] = 'human'\n",
    "trie['自然语言'] = 'language'\n",
    "trie['自语'] = 'talk\tto oneself'\n",
    "trie['入门'] = 'introduction'\n",
    "assert '自然' in trie\n",
    "\n",
    "# 删\n",
    "trie['自然'] = None\n",
    "assert '自然' not in trie\n",
    "\n",
    "# 改\n",
    "trie['自然语言'] = 'human language'\n",
    "assert trie['自然语言'] == 'human language'\n",
    "\n",
    "# 查\n",
    "assert trie['入门'] == 'introduction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2:3]=he\n",
      "[1:3]=she\n",
      "[2:5]=hers\n"
     ]
    }
   ],
   "source": [
    "# AC自动机\n",
    "from pyhanlp import *\n",
    "words = [\"hers\", \"his\", \"she\", \"he\"]\n",
    "ACTrie = JClass('com.hankcs.hanlp.algorithm.ahocorasick.trie.Trie')\n",
    "trie = ACTrie()\n",
    "for word in words:\n",
    "    trie.addKeyword(word)\n",
    "\n",
    "for emit in trie.parseText(\"ushers\"):\n",
    "    print(\"[%d:%d]=%s\" % (emit.getStart(), emit.getEnd(), emit.getKeyword()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1:4]=SHE\n",
      "[2:4]=HE\n",
      "[2:6]=HERS\n"
     ]
    }
   ],
   "source": [
    "# 双数组字典树AC自动机\n",
    "words = [\"hers\", \"his\", \"she\", \"he\"]\n",
    "map = JClass('java.util.TreeMap')()\n",
    "for word in words:\n",
    "    map[word] = word.upper()\n",
    "\n",
    "acdTrie = JClass('com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie')(map)\n",
    "\n",
    "for emit in acdTrie.parseText(\"ushers\"):\n",
    "    print(\"[%d:%d]=%s\" % (emit.begin, emit.end, emit.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8 Hanlp词典分词实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://picgogogo.oss-cn-hangzhou.aliyuncs.com/img/20200106115726.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoubleArrayTrieSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[江西/null, 鄱阳湖/null, 干枯/null, ，/null, 中国/null, 最大/null, 的/null, 淡水湖/null, 变成/null, 大草原/null]\n",
      "[上海市/null, 虹口区/null, 大连西路/null, 5/null, 5/null, 0/null, 号/null, S/null, I/null, S/null, U/null]\n",
      "[上海市, 虹口区, 大连西路, 550, 号, SISU]\n",
      "上海市 ns\n",
      "虹口区 ns\n",
      "大连西路 ns\n",
      "550 m\n",
      "号 q\n",
      "SISU nx\n"
     ]
    }
   ],
   "source": [
    "HanLP.Config.ShowTermNature = True\n",
    "segment = DoubleArrayTrieSegment()\n",
    "print(segment.seg(\"江西鄱阳湖干枯，中国最大的淡水湖变成大草原\"))\n",
    "\n",
    "## 传入自己的词典\n",
    "dict1 = HANLP_DATA_PATH +  \"/dictionary/CoreNatureDictionary.mini.txt\"\n",
    "dict2 = HANLP_DATA_PATH + \"/DICTIONARY/CUSTOM/上海地名.txt ns\"\n",
    "\n",
    "segment = DoubleArrayTrieSegment([dict1, dict2])\n",
    "print(segment.seg(\"上海市虹口区大连西路550号SISU\"))\n",
    "\n",
    "###  enablePartOfSpeechTagging()数字合并和词性标注功能合并，只有打开这个才能看到正确的词性\n",
    "segment.enablePartOfSpeechTagging(True)    # 激活数字和英文识别\n",
    "HanLP.Config.ShowTermNature = False\n",
    "print(segment.seg(\"上海市虹口区大连西路550号SISU\"))\n",
    "\n",
    "## 遍历分词结果\n",
    "segment.enablePartOfSpeechTagging(True)\n",
    "HanLP.Config.ShowTermNature = True\n",
    "for term in segment.seg(\"上海市虹口区大连西路550号SISU\") :\n",
    "    print(\"%s %s\" % (term.word, term.nature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AhoCorasickDoubleArrayTrieSegment 双数组AC自动机\n",
    "如果用户的词语都很长，使用AC自动机会更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江西 ns\n",
      "鄱阳湖 ns\n",
      "干枯 vi\n",
      "， w\n",
      "中国 ns\n",
      "最大 gm\n",
      "的 nz\n",
      "淡水湖 n\n",
      "变成 v\n",
      "大草原 nz\n"
     ]
    }
   ],
   "source": [
    "segment = JClass('com.hankcs.hanlp.seg.Other.AhoCorasickDoubleArrayTrieSegment')\n",
    "segment = segment()\n",
    "segment.enablePartOfSpeechTagging(True)\n",
    "\n",
    "for item in segment.seg(\"江西鄱阳湖干枯，中国最大的淡水湖变成大草原\"):\n",
    "    print(item.word, item.nature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.9 准确度评测\n",
    "## 1.混淆矩阵\n",
    "P(Positive), N(negative)\n",
    "\n",
    "|预测**\\**答案|P|N|\n",
    "|:--:|:--:|:--:|\n",
    "|P|TP|FP|\n",
    "|N|FN|TN|\n",
    "\n",
    "## 2. 精确度\n",
    "预测出来的P的正确率\n",
    "$$\n",
    "P = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P = \\frac{正确预测的阳(P)的数量}{预测结果为阳(P)的数量}\n",
    "$$\n",
    "## 3. 召回率\n",
    "正确预测的P占所有P的比率\n",
    "$$\n",
    "R = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P = \\frac{正确预测的阳(P)的数量}{所有的阳(P)的数量}\n",
    "$$\n",
    "## 4. F1值\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2 \\cdot P \\cdot R}{P + R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_path():\n",
    "    \"\"\"\n",
    "    获取测试数据路径，位于$root/data/test，根目录由配置文件指定。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(HANLP_DATA_PATH, 'test')\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def ensure_data(data_name, data_url):\n",
    "    root_path = test_data_path()\n",
    "    dest_path = os.path.join(root_path, data_name)\n",
    "    if os.path.exists(dest_path):\n",
    "        return dest_path\n",
    "    if data_url.endswith('.zip'):\n",
    "        dest_path += '.zip'\n",
    "    download(data_url, dest_path)\n",
    "    if data_url.endswith('.zip'):\n",
    "        with zipfile.ZipFile(dest_path, \"r\") as archive:\n",
    "            archive.extractall(root_path)\n",
    "        remove_file(dest_path)\n",
    "        dest_path = dest_path[:-len('.zip')]\n",
    "    return dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (2, 3), (3, 5)]\n",
      "P:91.80 R:95.69 F1:93.71 OOV-R:2.58 IV-R:98.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def to_region(segmentation: str) -> list:\n",
    "    \"\"\"\n",
    "    将分词结果转换为区间\n",
    "    :param segmentation: 商品 和 服务\n",
    "    :return: [(0, 2), (2, 3), (3, 5)]\n",
    "    \"\"\"\n",
    "    region = []\n",
    "    start = 0\n",
    "    for word in re.compile(\"\\\\s+\").split(segmentation.strip()):\n",
    "        end = start + len(word)\n",
    "        region.append((start, end))\n",
    "        start = end\n",
    "    return region\n",
    "\n",
    "# set & set 交集； set | set 并集； set - set 差集\n",
    "def prf(gold: str, pred: str, dic) -> tuple:\n",
    "    \"\"\"\n",
    "    计算P、R、F1\n",
    "    :param gold: 标准答案文件，比如“商品 和 服务”\n",
    "    :param pred: 分词结果文件，比如“商品 和服 务”\n",
    "    :param dic: 词典\n",
    "    :return: (P, R, F1, OOV_R, IV_R)\n",
    "    \"\"\"\n",
    "    A_size, B_size, A_cap_B_size, OOV, IV, OOV_R, IV_R = 0, 0, 0, 0, 0, 0, 0\n",
    "    with open(gold, encoding=\"utf-8\") as gd, open(pred, encoding=\"utf-8\") as pd:\n",
    "        for g, p in zip(gd, pd):\n",
    "            A, B = set(to_region(g)), set(to_region(p))\n",
    "            A_size += len(A)\n",
    "            B_size += len(B)\n",
    "            A_cap_B_size += len(A & B)\n",
    "            text = re.sub(\"\\\\s+\", \"\", g)\n",
    "            for (start, end) in A:\n",
    "                word = text[start: end]\n",
    "                if dic.containsKey(word):\n",
    "                    IV += 1\n",
    "                else:\n",
    "                    OOV += 1\n",
    "\n",
    "            for (start, end) in A & B:\n",
    "                word = text[start: end]\n",
    "                if dic.containsKey(word):\n",
    "                    IV_R += 1\n",
    "                else:\n",
    "                    OOV_R += 1\n",
    "    p, r = A_cap_B_size / B_size * 100, A_cap_B_size / A_size * 100\n",
    "    return p, r, 2 * p * r / (p + r), OOV_R / OOV * 100, IV_R / IV * 100\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(to_region('商品 和 服务'))\n",
    "\n",
    "    sighan05 = ensure_data('icwb2-data', 'http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip')\n",
    "    ## 词汇表\n",
    "    msr_dict = os.path.join(sighan05, 'gold', 'msr_training_words.utf8')\n",
    "    ## 没有分词的文章\n",
    "    msr_test = os.path.join(sighan05, 'testing', 'msr_test.utf8')\n",
    "    ## 以下两个是分词的结果\n",
    "    msr_output = os.path.join(sighan05, 'testing', 'msr_output.txt')\n",
    "    msr_gold = os.path.join(sighan05, 'gold', 'msr_test_gold.utf8')\n",
    "\n",
    "    ## 训练\n",
    "    DoubleArrayTrieSegment = JClass('com.hankcs.hanlp.seg.Other.DoubleArrayTrieSegment')\n",
    "    segment = DoubleArrayTrieSegment([msr_dict]).enablePartOfSpeechTagging(True)\n",
    "    \n",
    "    ## re.sub(\"\\\\s+\", \"\", line) 删除所有空白字符\n",
    "    with open(msr_gold, encoding=\"utf-8\") as test, open(msr_output, 'w', encoding=\"utf-8\") as output:\n",
    "        for line in test:\n",
    "            output.write(\"  \".join(term.word for term in segment.seg(re.sub(\"\\\\s+\", \"\", line))))\n",
    "            output.write(\"\\n\")\n",
    "    print(\"P:%.2f R:%.2f F1:%.2f OOV-R:%.2f IV-R:%.2f\" % prf(msr_gold, msr_output, segment.trie))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV & IV\n",
    "- OOV(Out Of Vocabulary) : 未登录词， 词典未收录的词\n",
    "- IV(In vocabulary) : 登陆词，词典已有的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 其他应用\n",
    "\n",
    "### 1. 停用词过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpype import JString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载双数组字典树\n",
    "def load_from_file(path):\n",
    "    map = JClass(\"java.util.TreeMap\")()\n",
    "    with open(path, encoding=\"utf-8\") as src:\n",
    "        for word in src:\n",
    "            word = word.strip()\n",
    "            map[word] = word\n",
    "    return JClass(\"com.hankcs.hanlp.collection.trie.DoubleArrayTrie\")(map)\n",
    "\n",
    "def load_from_words(*words):\n",
    "    map = JClass(\"java.util.TreeMap\")()\n",
    "    for word in words:\n",
    "        map[word] = word\n",
    "    return JClass(\"com.hankcs.hanlp.collection.trie.DoubleArrayTrie\")(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_termlist(termlist, trie):\n",
    "    return [term.word for term in termlist if not trie.containsKey(term.word)]\n",
    "\n",
    "def replace_stropwords_text(text, replacement, trie):\n",
    "    searcher = trie.getLongestSearcher(JString(text), 0)\n",
    "    offset = 0\n",
    "    result = ''\n",
    "    while searcher.next():\n",
    "        begin = searcher.begin\n",
    "        end = begin + searcher.length\n",
    "        if begin > offset:\n",
    "            result += text[offset: begin]\n",
    "        result += replacement\n",
    "        offset = end\n",
    "    if offset < len(text):\n",
    "        result += text[offset]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果： [停用, 词, 的, 意义, 相对而言, 无关紧要, 吧, 。]\n",
      "分词结果去除停用词： ['停用', '词', '意义', '无关紧要']\n",
      "不分词去掉停用词 停用词**意义**无关紧要**。\n"
     ]
    }
   ],
   "source": [
    "HanLP.Config.ShowTermNature = False\n",
    "trie = load_from_file(HanLP.Config.CoreStopWordDictionaryPath)\n",
    "\n",
    "text = \"停用词的意义相对而言无关紧要吧。\"\n",
    "segment = DoubleArrayTrieSegment()\n",
    "termlist = segment.seg(text)\n",
    "\n",
    "print(\"分词结果：\", termlist)\n",
    "print(\"分词结果去除停用词：\", remove_stopwords_termlist(termlist, trie))\n",
    "trie = load_from_words(\"的\", \"相对而言\", \"吧\")\n",
    "print(\"不分词去掉停用词\", replace_stropwords_text(text, \"**\", trie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 繁简转换\n",
    "简体s，繁体t，台湾tw， 香港hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言处理\n"
     ]
    }
   ],
   "source": [
    "CharTable = JClass(\"com.hankcs.hanlp.dictionary.other.CharTable\")\n",
    "print(CharTable.convert(\"自然語言處理\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在臺灣寫程式碼\n",
      "在台湾写代码\n",
      "在臺灣寫代碼\n",
      "在台湾写代码\n",
      "在臺灣寫程式碼\n"
     ]
    }
   ],
   "source": [
    "HanLP.convertToTraditionalChinese(\"自然语言处理\")\n",
    "HanLP.convertToSimplifiedChinese(\"自然語言處理\")\n",
    "\n",
    "print(HanLP.s2tw(\"在台湾写代码\"))\n",
    "print(HanLP.tw2s(\"在臺灣寫程式碼\"))\n",
    "print(HanLP.s2hk(\"在台湾写代码\"))\n",
    "print(HanLP.hk2s(\"在臺灣寫代碼\"))\n",
    "print(HanLP.hk2tw(\"在臺灣寫代碼\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
